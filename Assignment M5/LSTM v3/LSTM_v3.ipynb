{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBLXOZ2BDk1F",
        "outputId": "8d4f169f-68bd-4c87-80af-6087ed4b28fe"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time, copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from dataloader import EnglishSpanishDataset, train_test_val_split\n",
        "\n",
        "# Data loading\n",
        "words_file = os.path.join(os.getcwd(), 'annotations_file.csv')\n",
        "sentences_file = os.path.join(os.getcwd(), 'pairs.csv')\n",
        "\n",
        "words_df = pd.read_csv(words_file, header=None)\n",
        "sentences_df = pd.read_csv(sentences_file, header=None) # use smaller dataset to ease training time\n",
        "\n",
        "# Train, val and test split\n",
        "train_set, val_set, test_set = train_test_val_split(sentences_df, 0.8, 0.1, 0.1, random_state=42)\n",
        "\n",
        "# Define max_sentence_lenght\n",
        "max_len = 0\n",
        "for record in sentences_df.to_dict('records'):\n",
        "    current_len = len(record[0].split(' '))\n",
        "    if current_len > max_len: max_len = current_len\n",
        "max_sentence_length = max_len\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = EnglishSpanishDataset(words_df, train_set, max_sentence_length=max_sentence_length)\n",
        "val_dataset = EnglishSpanishDataset(words_df, val_set, max_sentence_length=max_sentence_length)\n",
        "test_dataset = EnglishSpanishDataset(words_df, test_set, max_sentence_length=max_sentence_length)\n",
        "\n",
        "# Defines batch size and device\n",
        "batch_size = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "dataloaders = {'train': train_dataloader,\n",
        "               'val': val_dataloader,\n",
        "               'test': test_dataloader}\n",
        "\n",
        "dataset_sizes = {'train': len(train_dataset),\n",
        "                 'val': len(val_dataset),\n",
        "                 'test': len(test_dataset)}\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, \n",
        "                 vocab_size: int, max_seq_length: int, dropout: float = 0.5):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=2, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedding = self.embedding(input)\n",
        "        output, hidden = self.rnn(embedding, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self, num_layers):\n",
        "        # We need two hidden layers because of our two layered lstm!\n",
        "        h0 = torch.zeros(num_layers, self.max_seq_length, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(num_layers, self.max_seq_length, self.hidden_size).to(device)\n",
        "        return (h0, c0)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def train_lstm(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
        "    best_loss = np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Each epoch has a training, validation, and test phase\n",
        "    phases = ['train', 'val', 'test']\n",
        "    \n",
        "    # Keep track of how loss evolves during training\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            for input_sequence, target_sequence in dataloaders[phase]: \n",
        "                \"\"\"\n",
        "                In a batch size of `10, the dataloader returns 10 examples\n",
        "                of input and target that both have 30 dimensions. So, each input\n",
        "                and target batch has (10, 30)\n",
        "                \n",
        "                The output of the model will be (10, 30, 39415), which is \n",
        "                (batch, sequence_length, n_classes).\n",
        "\n",
        "                For the loss function, it expects a 2D prediction and a 1D target.\n",
        "                For the prediction: (batch*sequence_length, n_classes)\n",
        "                For the target: (batch*sequence_length)\n",
        "\n",
        "                In this case, we have:\n",
        "                - predictions: torch.Size([300, 39415])\n",
        "                - targets:     torch.Size([300])\n",
        "                \"\"\"\n",
        "\n",
        "                # Now Iterate through each sequence here:\n",
        "\n",
        "                hidden = model.initHidden(num_layers=2) # Start with a fresh hidden state\n",
        "\n",
        "                current_input_sequence = input_sequence.to(device)\n",
        "                current_target_sequence = target_sequence.reshape(target_sequence.shape[0]*target_sequence.shape[1]).to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    loss = 0\n",
        "\n",
        "                    current_hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "                    output, hidden = model(current_input_sequence, current_hidden)\n",
        "                    output = output.reshape(output.shape[0]*output.shape[1], output.shape[2])\n",
        "                    l = criterion(output, current_target_sequence)\n",
        "                    loss += l\n",
        "\n",
        "                    # backward + update weights only if in training phase at the end of a sequence\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() / current_input_sequence.size(0)\n",
        " \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # deep copy the model if it's the best loss\n",
        "            # Note: We are using the train loss here to determine our best model\n",
        "            if phase == 'train' and epoch_loss < best_loss:\n",
        "              best_epoch = epoch\n",
        "              best_loss = epoch_loss\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        epoch_time_elapsed = time.time() - epoch_start\n",
        "        print(f'Epoch complete in {epoch_time_elapsed // 60:.0f}m {epoch_time_elapsed % 60:.0f}s')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model, training_curves\n",
        "\n",
        "lstm = LSTM(128, 128, vocab_size=len(train_dataset.words_df), max_seq_length=max_sentence_length).to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Train the model. We also will store the results of training to visualize\n",
        "word_rnn, training_curves = train_lstm(lstm, dataloaders, dataset_sizes, \n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(word_rnn.state_dict(), \"next_english_word.model\")"
      ],
      "metadata": {
        "id": "5Hq4cj00DIvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWLH7cd_J_TR"
      },
      "outputs": [],
      "source": [
        "def predict(model, input_sentence, max_length = 20):\n",
        "  output_sentence = input_sentence + \" \"\n",
        "  input_tensor, target_tensor = train_dataset.create_input_and_target_tensor(input_sentence)\n",
        "  h0 = torch.zeros(2, 128).to(device)\n",
        "  c0 = torch.zeros(2, 128).to(device)\n",
        "  hidden = (h0, c0)\n",
        "  # hidden = word_rnn.initHidden(num_layers=2)\n",
        "  current_input_sequence = torch.reshape(input_tensor, (-1,1)).to(device)\n",
        "  input = None\n",
        "\n",
        "  for i in range(current_input_sequence.size(0)):\n",
        "    output, hidden = word_rnn(current_input_sequence[i], hidden)\n",
        "\n",
        "  output_tensor = [int(value) for value in input_tensor]\n",
        "  output_tensor.append(int(output.argmax()))  \n",
        "  print(output_tensor)\n",
        "  print(train_dataset.tensor_to_sentence(output_tensor))\n",
        "  return output_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO5z6b2zKIBV"
      },
      "outputs": [],
      "source": [
        "input_sentence = \"bone\"\n",
        "output_tensor = predict(word_rnn.to(device), input_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH140SxsM-5M"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(training_curves, \n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                plt.plot(epochs, training_curves[key])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHPLKkcSNBe3"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}